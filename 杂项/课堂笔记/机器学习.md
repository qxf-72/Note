# Introduction

**人工智能与机器学习**
- 没有明确的定义，ML 是 AI 的一个子集。
-
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-11_170459.jpg" width = 500 /> </div>

- <span style="background:#fff88f">AI 强调拥有自己的智能，ML 侧重于能够自己学习</span>。

<br/>


---

**Machine Learning = 寻找合适的模型（函数）**
- 框架：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-11_170952.jpg" width = 500 /> </div>

机器学习流程：
<div align="center"> <img src=" https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-11_171517.jpg" width = 500 /> </div>

- 训练数据：特征+标签。
- 模型：一系列函数的集合，机器学习的目的就是找到最合适的那个函数。
- 模型评估：损失函数+代价函数。
- 优化：加速寻找最优函数 $f^*$ 的过程。

<span style="background:#fff88f">并不是所有 ML 都符合以上流程，例如 无样本学习</span>。

---

<br/>


**机器学习分类**
- 监督学习（Supervised Learning）：已知数据和其一一对应的标签
	- 回归 Regression
	- 分类 Classification
- 非监督学习 （Unsupervised Learning）
	- 聚类 Clutersing
	- 降维 Dimensionality reduction
- 集成学习 resemble learning：按照某种算法生成多个模型，如分类器或者称为专家，再将这些模型按照某种方法组合在一起来解决某个智能计算问题。
- 强化学习
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-11_181526.jpg" width = 600 /> </div>

---

<br/>

<br/>

<br/>

<br/>





# Numpy

**创建 array**
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-11_181848.jpg" width = 600 /> </div>

**创建 matrix**
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-11_182117.jpg" width = 600 /> </div>

**和创建向量相比，创建矩阵需要多加一对括号。**

---


<br/>


<br/>


<br/>


<br/>



# Linear Regression

线性回归是一个回归算法。

**何为线性？**
无论特征值有多少维度，特征值和标签的关系是线性的：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_180132.jpg" width = 600 /> </div>


<br/>


## Lost/Cost Function
**损失函数**：代表一个样本带来的偏差。**公式中的 $1/2$ 是为了之后梯度下降法的计算更简洁。**

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_181122.jpg" width = 300 /> </div>

**代价函数**：所有随时函数的总和。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_181405.jpg" width = 400 /> </div>

---

<br/>


<br/>


<br/>


## 特征空间和参数空间
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_181658.jpg" width = 700 /> </div>
特征空间中的一条直线对应着参数空间中的一个点。梯度下降方法，就是根据梯度来在参数空间中求的使得代价最小的模型（函数）。

---

<br/>


<br/>


<br/>



## Gradient Descent
**梯度**：代价函数对于每个参数的偏导数组成的向量，其指引了代价函数**上升最快**的方向。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_182359.jpg" width = 400 /> </div>

**优化**：参数减去梯度
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_182451.jpg" width = 500 /> </div>

超参数 $\alpha$ 控制了优化的速度，其过大过小都不好：
- $\alpha$ 太小：优化速度太慢
- $\alpha$ 太大：反复横跳，无法到达最优
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-26_182733.jpg" width = 700 /> </div>

---

<br/>

<br/>



## 梯度的计算方式
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_083710.jpg" width = 400 /> </div>

在实际编写代码过程中，对于常数项 b 可以在 X 中增加一项 **偏置项** ，方便统一处理，[[机器学习#^fafff7|参考此处]]。

---

<br/>


<br/>


<br/>



## 梯度下降法缺点

对于多峰问题，梯度下降法可能会陷入局部最优解，并不能确保找到全局最优解。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_084405.jpg" width = 700 /> </div>

---

<br/>


<br/>


<br/>



## 特征归一化 Normalize feature
对于多个特征不在同一尺度的问题，此时 $\alpha$ 将很难确定，此时需要对特征进行归一化。
<div align="center"> <img src=" https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_091048.jpg" width = 300 /> </div>


<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_090721.jpg" width = 700 /> </div>


---

<br/>


<br/>


<br/>


<br/>



# Logistic Regression

逻辑回归是一个**二分类算法**。

分类问题：
- Binary classification：需要分类的类别只有两种，逻辑回归用于处理二元分类。
- Multiclass classification：多个（>2）类别。

<br/>


## 逻辑回归模型
在二元分类算法中，由于标签取值只有两种（0 和 1），而线性函数的取值为 R，所以需要 **Sigmoid 函数**进行转换，如下图：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_095302.jpg" width = 600 /> </div>

观察 Sigmoid 函数可知，该函数分为了两个部分，并且在大于 4 或者小于-4 时，斜率很小。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_095802.jpg" width = 400 /> </div>

---

<br/>

<br/>



## 二元交叉熵损失函数  Binary Cross-entropy loss
以下为在二元情况下，交叉熵损失函数的公式：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_100405.jpg" width = 600 /> </div>

---

<br/>


<br/>


<br/>




## 逻辑回归梯度计算
逻辑回归和线性回归梯度一样。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_100651.jpg" width = 700 /> </div>

在编写代码时，**对于常数项，可以在 X 的每个样本增加一项值为 1 的特征，方便统一处理**，梯度的计算方法如下： ^fafff7
```py
# 方便计算偏置值，即常数项
X = np.concatenate([np.ones([n, 1]), data[:, 0:6]], axis=1) 
Y_hat = 1 / (1 + np.exp(-X @ W))       # Sigmoid function
G = -np.dot(X.T, Y - Y_hat) / n        # Gradient
```

> np. dot 函数处理矩阵乘法
> 而 * 操作符则执行数乘

---

<br/>

<br/>


<br/>




## 数值稳定问题
在计算损失函数时，当输入损失函数的值很将近于 0（期待值为 1）时，此时得出的损失将非常大，有可能会造成溢出问题。

所以，可以对损失函数进行优化，方法如下，**将 Sigmoid 函数带入交叉熵损失函数中**，而不是先算出预测值后代入。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_103920.jpg" width = 500 /> </div>



---

<br/>

<br/>

<br/>

<br/>



# Model Selection

## 欠拟合和过拟合
在特征空间不变时，参数越多，参数空间越大，那么模型越复杂：
- 参数过少时，模型过于简单，不能很好拟合目标模型，欠拟合。
- 参数太多是，模型变得很复杂，当训练集太小时，过拟合。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_113929.jpg" width = 700 /> </div>

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_114029.jpg" width = 700 /> </div>

---

<br/>

<br/>


<br/>




## 训练集和验证集
模型训练之后，还需要使用测试集进行测试，以下为几种不同的情况（红线为验证集测试情况）：
- 欠拟合
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_115103.jpg" width = 300 /> </div>

- 拟合良好
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_115320.jpg" width = 300 /> </div>

- 过拟合
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_115428.jpg" width = 300 /> </div>

---

<br/>


<br/>

<br/>




## 数据预处理 Data Preparation
因为无法获得全部样本空间，无法无限的收集数据，所以一般把数据集分为：**训练集**和**验证集**：
- 训练集 trainning set：作为模型训练的数据。
- 验证集 validation set：在模型训练之后对模型进行测试，评估模型的好坏。
- **测试集 test set：并不会训练模型的数据的一部分**，而是模型在应用时处理的处理。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_120130.jpg" width = 500 /> </div>

---

<br/>


<br/>


<br/>




## N 则交叉验证 N-fold Cross Validation
N 则交叉验证是将数据集分为 N 份，分别将每份作为验证集，其余组合起来作为训练集，则可以进行 N 次验证，目的是为了避免运气成分。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_120528.jpg" width = 400 /> </div>

---

<br/>


<br/>


<br/>




## 偏差和方差 Bias and Variance

**偏差（Bias）体现的是模型的拟合能力**，**方差（Variance）体现的是模型的鲁棒性**。

- 偏差：衡量预测值与真实值的关系, 是指预测值与真实值之间的差值。
- 方差：衡量预测值之间的关系,和真实值无关.也就是他们的离散程度。


<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-09-27_120902.jpg" width = 400 /> </div>


- 偏差小，方差大：过拟合问题。
- 偏差大，方差小：欠拟合问题。
- 偏差和方差都很大，可能是数据集的问题。

---

<br/>


<br/>


<br/>

<br/>




# Softmax Regression
Softmax Regression 是 Logistic 回归的推广，Logistic 回归是处理二分类问题的，而 Softmax Regression 是处理**多分类问题**的。


<br/>



## One-hot Vector
在处理二元分类问题时，由于只有两种类别，所以使用 0 和 1 来代表两种类别，但是在多分类（类别多于 2）问题中，则不能使用 0,1,2 这样的自然数来代表类别：这样的编码方式存在隐含关系——相邻数字代表的类别更相近，但是实际问题不一定如此。

为此，需要使用独热编码，n 分类问题，使用一个长度为 n 的向量来代表一个类别，每一个向量中，有且只有一个 1，其余全为 0。


---

<br/>

<br/>

<br/>


## Softmax 回归模型
在 Softmax 回归中，**有 n 个类别，就分别用 n 个模型（函数）来预测该样本属于该类别的概率**，最后得到 Z 向量，经过 **Softmax 函数**处理得到 Y_hat，具体如下：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-04_154021.jpg" width = 600 /> </div>

---

<br/>


<br/>

## 交叉熵损失函数 Cross-entropy loss

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-04_154933.jpg" width = 600 /> </div>

可以看出，损失函数的结果为独热向量中为 1 的那项对应的概率的 log 值，然后取负值，这与二元交叉熵损失函数一样。

---


<br/>


<br/>


<br/>



## Softmax 回归梯度下降

梯度计算公式如下：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-04_160659.jpg" width = 400 /> </div>

详细计算公式如下：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-04_163303.jpg" width = 600 /> </div>

可以看出，梯度计算公式和线性回归、逻辑回归的梯度计算公式高度相似。

> 尽管线性回归和逻辑回归使用不同的损失函数，但它们的梯度计算形式可以看起来相似——它们都涉及到求解模型参数 θ 以最小化损失函数。
> 无论是线性回归的 MSE 损失还是逻辑回归的交叉熵损失，它们的梯度计算**都涉及到计算损失函数对模型预测的误差（即残差）的影响**，然后将这个误差乘以输入特征，最终计算出梯度。由于这个计算过程的一般性质，使得梯度计算的形式在某种程度上看起来相似，尽管损失函数本身是不同的。

---

<br/>


<br/>


<br/>


<br/>


# Regularization

正则化可以**缓解（并不能）过拟合问题**。

对于一个多项式的模型，自变量（特征）出现的次数控制模型的复杂度（自变量出现次数多，则说明参数数量会多于特征数量）。**当参数数量远大于特征数量时，很可能出线过拟合问题。**

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_144231.jpg" width = 600 /> </div>

<br/>

## $L^2$ Regularization
正则化的本质就是**在代价函数中增加一个罚项**，模型越复杂，代价越大。

在 $L^2$ Regularization 中，增加的罚项如下：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_145300.jpg" width = 300 /> </div>

此时，代价函数变为：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_145434.jpg" width = 600 /> </div>

在上式中，$\lambda$ 作为超参数控制避免出现欠拟合问题。$L^2$ Regularization 会主动向着使得模型更简单的方向前进，需要 $\lambda$ 进行控制。

<br/>

 $L^2$ Regularization 的梯度计算：
 
 <div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_150135.jpg" width = 600 /> </div>

---

<br/>


<br/>


<br/>



## $L^1$ Regularization

类似的， $L^1$ Regularization 添加罚项为：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_150547.jpg" width = 250 /> </div>


$L^1$ Regularization 容易让某些参数变为 0，如下图所示。**$L^1$ Regularization 可以作为特征选取器**。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_150707.jpg" width = 400 /> </div>

---

<br/>


<br/>


<br/>

<br/>

# Neural Networks

## Neuron

**线性神经元**
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_151441.jpg" width = 500 /> </div>

**神经元**
线性神经元要经过**激活函数**处理，去除线性，才能得到神经元。因为这样在组成神经网络时才有意义，线性神经元组成的神经网络，本质上可以用一个线性神经元表示（参数带入化简）。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_151654.jpg" width = 200 /> </div>

**神经网络**
即，神经元组成的网络：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_151743.jpg" width = 700 /> </div>

---


<br/>


<br/>


<br/>



## Activation Function
下图为不同类型的激活函数，其中**最常用的是 ReLU 函数**，因为在进行求导时，求导结果是 1 或者 0，可以**缓解 梯度消失 和 梯度爆炸 问题**。

> 在深层的神经网络中，**求导结果总是大于 1，那么梯度呈指数增长**。当**求导结果总是小于 1 时，梯度将越求越小，出现梯度消失问题**。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-18_152112.jpg" width = 700 /> </div>

---


<br/>


<br/>


<br/>



## Fully Connected Neual Network

全连接神经网络示意图如下，其中 $a^{[0]}$ 代表第 0 层的输出，其实就是初始的特征。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-19_150532.jpg" width = 600 /> </div>

其中每一层的组成如下图所示：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-19_151110.jpg" width = 600 /> </div>

神经网络的输出层和 [[#Softmax 回归模型]] 一样，神经网络除了输出层之外的层的作用——==提取特征==。

---

<br/>


<br/>

## Loss/Cost Function
神经网络使用 [[#交叉熵损失函数 Cross-entropy loss]]，在 Softmax 回归中已经给出，不再阐述。

---

<br/>


<br/>

## Gradient Descent
首先考虑最后一层的，由于最后一层和 Softmax 回归是一样的，所以计算结果也和 [[#Softmax回归梯度下降]] 一样。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-19_152440.jpg" width = 700 /> </div>


接下来，需要反向去求之前层的梯度，这就是——==**Back Propagation 前向传播**==。 ^ed008b

通过下面的公式可以看到——==上一次的 $\frac{\partial L}{\partial z^{\left[ L-1 \right]}}$ 可以由下一层的 $\frac{\partial L}{\partial z^{\left[ L \right]}}$ 和 $w^{\left[ L \right]}_{m,i}$ 进行递推，所以往前递推过程中需要保存每一层的 $\frac{\partial L}{\partial z}$==。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-19_153039.jpg" width = 700 /> </div>

---

<br/>


<br/>

## 公式推导过程
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/e2760cbf4d7a949a95e28e1e65ec4a0.jpg" width = 400 /> </div>

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/915ea7a95ad7e4d13697bbbe1f8b148.jpg" width = 400 /> </div>

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/9b5e3bb53c60af100a626fbc5df463b.jpg" width = 400 /> </div>

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/310c15a92013a5b4e23a3aab29b9327.jpg" width = 400 /> </div>

---

<br/>


<br/>


<br/>


<br/>


# Optimization

## GD 缺点
- 可能会陷入局部最优点、鞍点 (saddle point)、
- 在梯度平缓位置 plateua 更新缓慢。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_152350.jpg" width = 500 /> </div>

- 参数尺度不一
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_152714.jpg" width = 500 /> </div>

---

<br/>


<br/>

## GDM
GDM （GD with Momentum），顾名思义，就是带有势头的 GD，公式如下：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_153020.jpg" width = 300 /> </div>

- 一般而言， $\beta =0.9$ 。
- $V_0=0$

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_153224.jpg" width = 600 /> </div>

从上图可以看出，GDM 可以缓解局部最优解问题。

从公式形式可以看出，随着更新，之前的 $V_t$ 在当前 $V$ 的占比会越来越小。这种方法成为——==指数滑动平均 Exponential Moving Average==。

---

<br/>


<br/>

## AdaGrad
Adaptive Gradient 是一种不完善的，可以根据参数梯度大小来调整更新速率的算法。公式如下：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_153921.jpg" width = 300 /> </div>

从公式可以看出， $S_t$ 会随着更新越来越大，后期基本没有调整速率的效果。更坏的情况下，在一开始就遇到一个很大的梯度， $S_t$ 在开头就变得很大。

---

<br/>


<br/>

## RMSProp
Root Mean Square Propagation 均方根传播，是==使用指数滑动平均来对 AdaGrad 进行改善的算法==，公式如下：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_154411.jpg" width = 300 /> </div>

还是使用 $S_t$ ，不过 $S_t$ 的更新方式使用了指数滑动平均。

- 一般而言， $\beta =0.9$ ， $\varepsilon =10^{-6}$ ， $s_0=0$

---

<br/>


<br/>


## Adam
Adaptive Moment Estimation 自适应矩估计，是结合了 GDM 和 RMSProp 的算法，公式如下：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_155018.jpg" width = 700 /> </div>

可以看出，Adam 中的 V_hat 和 S_hat 是新的，作用是解决算法启动慢的问题。

- 一般而言， $\beta_1 =0.9$ ， $\varepsilon =10^{-6}$ ， $\beta_2 =0.999$
- S 和 V 初始都为 0。

---


<br/>


<br/>

## 其他 GD 算法
由于计算机内存是有限的，如果数据集太大，那么就无法一次性读入内存中，所以有以下 GD 算法，其原理就是——每次读入部分样本，学习完这部分样本，再读入样本进行学习。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-10-26_155703.jpg" width = 400 /> </div>

---

<br/>


<br/>


<br/>


<br/>


# Convolutional Neural Networks

## 卷积神经网络层


<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-01_200842.jpg" width = 700 /> </div>


<br/>


可以从上图的右侧图片可以看出，全连接神经网络和卷积神经网络的区别。卷积神经网络每一层的一个输出对应的是部分输入的卷积，而全连接神经网络一个输出是所有输入计算的结果。


---

<br/>


<br/>


<br/>

## Padding

经过卷积计算之后，输出层会比输入层小。为了避免这种情况，需要对输入层进行 padding 操作，使得输出层和原来的输入层尺寸一致。

输出层和输入层的尺寸关系如下图框出公式所示：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-01_201507.jpg" width = 700 /> </div>

<br/>



当需要填充两层时，最好向上图那样 padding，否则如果都填到一侧而不是这样包围一圈（均匀分散），信息损失量太大。

---

<br/>


<br/>


<br/>

## 卷积核

卷积是有意义的，特别是在处理数字图像图片方面，以下是一个例子：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_064613.jpg" width = 700 /> </div>

---

<br/>


<br/>


<br/>

## Pooling

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_065119.jpg" width = 400 /> </div>


当输入图像经过卷积处理之后，在使用 Relu 激活函数处理，得到的图像可能会有很多 0 值，计算这些 0 值是没有意义的，需要进过 pooling 操作合并信息。

pooling 操作和卷积操作类似，但是计算方式不同、==pooling kernel 的步长和其尺寸是一样的==。有以下两种 pooling 策略：
- 取最大值
- 取平均值

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_065623.jpg" width = 700 /> </div>

---

<br/>


<br/>


<br/>


## Convolutional Neural Networks


<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_065926.jpg" width = 700 /> </div>

卷积神经网络最后一层是全连接神经网络层。计算损失的方法和之前一样——交叉熵损失函数。

---

<br/>


<br/>


<br/>


## Back Propagation

由于最后一层是和[[#^ed008b|全连接神经网络]] 是一样的，所以偏导计算方式也一样。所以 $\frac{\partial L}{\partial Z}$ 可以求，不再讨论。

<br/>



**Pooling**
由于 Z 是进过 Pooling 处理的，在本例中 pooling 是选取一个最大的数，在进行正向传播时，需要把 pooling 每一个区域的最大数的位置记录。

这样就可以得到 $\frac{\partial z}{\partial x}$ ，即有标记的位置的偏导对应到 $\frac{\partial L}{\partial Z}$ ，其余都为 0，如下图：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_073907.jpg" width = 700 /> </div>


<br/>

**ReLU**
由 ReLU 的计算公式可以知道，很容易进行反推：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_080159.jpg" width = 500 /> </div>



<br/>

**卷积**
卷积反向传播 计算方式有两种：
- 从列的角度考虑：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-02_080748.jpg" width = 700 /> </div>


- 从行的角度考虑：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_104700.jpg" width = 700 /> </div>

---

<br/>

之前所讲的都是最后一层的反向传播， $\frac{\partial L}{\partial w}$ 已经求出，此时需要求出 $\frac{\partial L}{\partial x}$ ，这里的 $x$ 是最后一层的输入，同时也是前一层的输出，求出偏导才可以继续反向传播下去。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_105345.jpg" width = 600 /> </div>



求 x 的偏导比较麻烦，如下图：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_105930.jpg" width = 300 /> </div>

经过观察可以得出规律：先把做一个 Padding，把卷积核反转。做卷积操作就可以得到 x 的偏导。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_110201.jpg" width = 600 /> </div>


---

<br/>


<br/>


<br/>

## Image to Column

正常做卷积操作时，需要 for 循环，相比较于高度优化之后的矩阵运算，for 循环太低。为了用矩阵运算来代替 for 循环，可以对图像或者卷积核进行处理：

两种不同的处理策略：

- 对图像进行操作：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_121115.jpg" width = 600 /> </div>

- 对卷积核进行操作：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_121733.jpg" width = 600 /> </div>


---

<br/>


<br/>


<br/>


<br/>

# Perceptron

## 线性二分类算法

感知机是用于线性二分类的算法，其特点如下图所示。感知机是一种很早就提出来的算法。
- <span style="background:#fff88f">y 的取值为 -1 和 1</span>。

<br/>


<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_121949.jpg" width = 600 /> </div>


---

<br/>


<br/>


<br/>

## Distance

<span style="background:#fff88f">感知机是通过距离来判断分类结果</span>。为了简化运算，其又把距离 d 做了处理，因为**只需要知道结果的正负**。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_122322.jpg" width = 600 /> </div>

---

<br/>


<br/>


<br/>

## 损失函数

只对分错类的样本计算损失函数，所以需要乘以 y，**如果结果是负数，则说明分类错误**。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_122701.jpg" width = 600 /> </div>

---


<br/>


<br/>


<br/>

## 优化

感知机的优化方法是**单样本优化**，即一次只输入一个样本。

即使样本能分类，感知机一般也不能很好的进行分类。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-10_123659.jpg" width = 600 /> </div>

---


<br/>


<br/>


<br/>


<br/>

# Support Vector Machine

支持向量机（support vector machines, SVM）是一种**二分类模型**，它的基本模型是定义在特征空间上的**间隔最大**的线性分类器。

## Max Margin Problem

**支持向量**：距离超平面最近的样本点，一般每一类样本都只有一个支持向量。
**间隔 Margin**：即支持向量直线的距离，支持向量机就是要求出能够是的间距最小的超平面。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-15_202951.jpg" width = 500 /> </div>

经过化简，可将问题转化为：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-15_203421.jpg" width = 500 /> </div>


---

<br/>


<br/>


<br/>

## 公式推导

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/e1cacdb02634cdde7eaa2c3d54f5fd4.png" width = 500 /> </div>

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/7e1ec074d92b1b5363e50f643a9685d.png" width = 500 /> </div>

**互补松弛条件**：当 $g_i>0$ 是，即不是支持向量，有 $\lambda _i=0$ 。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2668fdd55ef3e3f87de3e0197ad9343.png" width = 500 /> </div>


<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/32e795c5c4de7d38e5d3a1467a1d231.png" width = 500 /> </div>

**一个简单例子**
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-11-25_092141.jpg" width = 700 /> </div>


---

<br/>


<br/>


<br/>


<br/>


# Decision Tree

决策树是用于特征和标签的取值都是离散的值（无法求导）的样本的，使用了贪心思想，的分类算法。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_152258.jpg" width = 400 /> </div>


<br/>


<br/>


## Suprise & Entropy

**Suprise 惊讶程度**，用于描述——分类之后，对于某个标签的出现的惊讶程度。当标签占据的比例越大，惊讶程度就越小：

$$
S\left ( u_i \right) =log \frac{1}{P\left ( u_i \right)}
$$


**Entropy 熵**就是惊讶程度的数学期望：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_153009.jpg" width = 500 /> </div>


计算熵的例子：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_153159.jpg" width = 700 /> </div>

其中 log 是以 e 或者 2 为底数。

---

<br/>


<br/>


<br/>


## ID 3

**信息增益 Information gain**
信息增益定义如下，即 **分类前的熵-分类后的熵**。信息增益越大，说明分类效果越好。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_153447.jpg" width = 300 /> </div>

**ID 3 使用 IG 作为贪心的依据**。



**ID 3的缺点**
- 信息增益偏向于选择有更多取值的 feature 作为分类依据。
- ID 3 只能用于离散的情况。
- ID 3 不支持剪枝。


---


<br/>


<br/>


<br/>



## C 4.5

**信息增益比 Information gain ratio**

IGR 定义如下，由于 除了分类前的熵，所以不会出现更偏爱取值多的 feature 的情况。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_154119.jpg" width = 300 /> </div>

C 4.5 采 IGR 作为贪心的依据。


<br/>


**取值连续的情况**
对于取值连续的情况，可以进行**离散化**操作，例子如下：

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_154535.jpg" width = 700 /> </div>



<br/>

**C 4.5 缺点（特点）**
- 不支持在连续的值上面进行回归。
- 构建的是多叉树（在一层上面，可能会有很多分支）。
- 包含许多 log 计算（计算复杂度高）。


---


<br/>


<br/>


<br/>

## 剪枝

### pre-pruning

以 cout 最大的取值作为答案，使用一个验证集进行验证，得出准确度。**pre-pruning 边建树边剪枝，如果分支之后的准确度更大，就分支，否者就不进行创建分支。**

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_175806.jpg" width = 700 /> </div>


<br/>


<br/>

### post-pruning

**post-pruning 在树已经建立之后，从叶子节点开始评估**，如果删除当前分支会使得准确度提升，就删除分支。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_180441.jpg" width = 700 /> </div>


---


<br/>


<br/>


<br/>


<br/>


## CART

CART（Classification and Regression Tree ）构建出的树为**二叉树**，使用**基尼指数 Gini Index**对分类效果进行评估。

**Gini Index**
基尼指数越小越好。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_183747.jpg" width = 500 /> </div>


**特征取值情况大于 2**
把取值情况分为两类（一类情况中的取值数大于 1）即可。


---


<br/>


<br/>


<br/>


<br/>


# Ensemble Learning

集成学习，即多个模型进行学习，将学习结果按照一定的方式进行组合。按照组合方式的不同可以分为两类：
- Bagging
- Boost


<br/>

## Bagging

**Bagging基本概念**
- Bagging 是 bootstrap aggregating 的缩写
- Bagging 可以**降低方差**，而不能降低偏差
- 是一种**框架**，而不是一种具体算法
- 可以**并行**训练模型

<br/>


**Bagging 流程**
- 需要多少个模型，就进行几次放回抽样，从训练集中抽取出样本。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_204802.jpg" width = 600 /> </div>

- 将结果取平均
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_204918.jpg" width = 600 /> </div>



<br/>

**Random Forests**
随机森林，即一组 Bagged 的决策树。在进行抽样时，不仅对行进行抽样，还对列进行抽样，如下：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_205325.jpg" width = 700 /> </div>


---


<br/>


<br/>


<br/>


## Boosting

**Boosting 基本概念**
- 另一种集成学习的**框架**
- **串行**训练模型
- **降低偏差**，不能降低方差
- 包含多个 weak learners

<center>
<img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_210745.jpg" width="500" />
</center>




<br/>

### AdaBoosting 

AdaBoosting 是具体的一种Boosting 算法。

**AdaBoosting 模型**
- 对于每一个样本点，模型输出为：每一个子模型的输出 $\times$ 各自系数 $\alpha _t$ 。
- 模型的系数，由模型的错误率决定。错误率 $e_t$ 越低，系数越大。
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_210048.jpg" width = 400 /> </div>


<br/>

**损失函数和代价函数**
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_210604.jpg" width = 600 /> </div>


可以将代价函数进行分解——可以将之前所有 learner 构成的模型的损失 看作是：当前的 learner 对于该样本点重要程度。
（如果之前已经对于该样本做得很好了，则损失小，当前对于该样本的影响不需要大）。

<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_211200.jpg" width = 600 /> </div>


继续对代价函数化简，再令对 $\alpha_t$ 求导为 0，解方程可以求出 $\alpha_t$ 。

<center>
<img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_212729.jpg" width="600" />
</center>



<br/>

**AdaBoosting 算法流程**

<center>
<img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_213534.jpg" width="600" />
</center>


计算例子：
<div align="center"> <img src="https://picture-in-md.oss-cn-guangzhou.aliyuncs.com/2023-12-19_214725.jpg" width = 400 /> </div>






---

